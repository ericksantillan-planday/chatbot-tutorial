{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3627005-2b44-4e88-8589-373db1c21153",
   "metadata": {},
   "source": [
    "# RAG: putting everything together\n",
    "\n",
    "In this notebook we will put all the building blocks together to have our own RAG application\n",
    "\n",
    "## Bringing back all of our work from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e93755-90fc-4440-bead-a4e4e92759f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ac6e5-25ab-4ade-ab4d-cd28dc44dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_deployment=\"\"\n",
    "api_key=\"\"\n",
    "openai_api_version=\"2024-02-01\"\n",
    "azure_endpoint=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b461df41-5405-4a81-9f12-de2d98bb406c",
   "metadata": {},
   "source": [
    "### Define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0405a1-193e-4bac-9f34-2e5047be3f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35 = AzureChatOpenAI(\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_key=api_key,\n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n",
    "\n",
    "llm=gpt_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dcb62d-d504-4892-9b6e-0f81304a68b7",
   "metadata": {},
   "source": [
    "### Load our vectorsore and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02278d-bd4f-4d62-a0b9-1d6678fe9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.load_local(\"vector_store\", embeddings=embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac7409-9aad-43ba-a343-dbdb235806c4",
   "metadata": {},
   "source": [
    "## Retrieval and Generation: Retrieve\n",
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "First we need to define our logic for searching over documents. LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "\n",
    "The most common type of [Retriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/) is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facilitate retrieval. Any VectorStore can easily be turned into a Retriever with `VectorStore.as_retriever()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec434fee-4ab3-4f35-9097-f2a9b2a9ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c664509-22c4-40ff-b332-3c610b59de53",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are revenue units?\"\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318f79b-656e-4e72-8753-ac9d8dd0faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c290c0f8-f67b-4391-a149-2e3153ec639e",
   "metadata": {},
   "source": [
    "## What's happening under the hood?\n",
    "It may seem really obscur what one line of code is doing but it's really simple. It's a 4 step process:\n",
    "1. The `query` is passed through our embedding model and gets transformed into a vector, let's called it `query_vector`\n",
    "2. The `query_vector` is then compared to all the vectors in the vectorstore. Remember that those vectors in the vectorstore are just a mathematical representation of parts of the documents\n",
    "3. We then take the vectors that are the most \"similar\" to our `query_vector`\n",
    "4. We return a list with the documents that had the nearest distance to the `query`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5c5a80-e05f-49ff-9cde-6e8454368c6e",
   "metadata": {},
   "source": [
    "# Retrieval and Generation: Generate\n",
    "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
    "\n",
    "Let's start by defining the message we will send to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfd20b-67a3-4029-b62e-4ead56f2eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5095ed2-3323-4f1f-8b80-d3bf9abe61d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df45ddd-a03d-453d-bb48-c92b7a8b0fa9",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "We will create a chain called `rag_chain` that will have only one input: the user's `question`.\n",
    "\n",
    "The `question` be forked and passed through two different pipelines:\n",
    "1. The retrieval pipeline, where the question will be compared to the documents inside the vectorstore using the `retriever` and its output will be appended usint the `format_docs` function. The output of this chain will be a string and be passed to `prompt` on the `context` property.\n",
    "2. The `question` will be other property passed to the `prompt`.\n",
    "\n",
    "Once the prompt is filled with context and the question, we will send it to the `llm`, and we will print out the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1cb41-2416-43ba-becb-5204383a8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4edd3-3079-4d59-9f5b-416314f22fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0b29b-31d7-459d-b8a1-38b661a6b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"what is a revenue unit?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74d083-45d0-418b-9ec0-1362be4c58cc",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "\n",
    "Now it's up to you, here we propose some exercises for you to play with, feel free to mess around with it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6267f9-94f9-4a02-9ae0-f59cf9fd015a",
   "metadata": {},
   "source": [
    "# Exercise 1: Validation\n",
    "Right now our agent can answer questions about Planday... but also about coding in python, or about the weather in Mexico. I think you can see how this can be abused... How can you put some guard rails to avoid it?\n",
    "\n",
    "Maybe modify the prompt... maybe separate it into two prompts... who knows\n",
    "\n",
    "The following prompt shouldn't be possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4a2a8-92e9-49ff-973b-99b9a58ebb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_chain.invoke(\"Write a python function that somes all fibonacci numbers between 1-18\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a90ad-b6fa-4796-95d0-ae8608ecd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c45298-0beb-4662-b50c-5d52aa4ae7f0",
   "metadata": {},
   "source": [
    "# Exercise 2: Follow-up questions\n",
    "Right now, our agent can answer questions about Planday. But if you ask a follow up question, it has no idea about what you were talking about as an LLM has **no memory**. The only way to provide it with memory is by somehow adding the past requests manually to the request. How could you do it...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a175d1-12a5-46ed-915d-1258e59b8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dddfb3c-338e-4187-bf60-5483878724d1",
   "metadata": {},
   "source": [
    "# Exercise 3: Cite your sources!\n",
    "We know LLMs are prompt to hallucinate... how can you make it retourn the sources of where the knowledge came from?\n",
    "\n",
    "Pssst: maybe you want to look into modifyin the `format_docs` function, although there are several ways of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acf930-5c83-40b1-ae16-98a58600311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-tutorial",
   "language": "python",
   "name": "chatbot-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
