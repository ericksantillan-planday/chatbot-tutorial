{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80674d7f-39f2-4a30-ad11-16ff73eedb82",
   "metadata": {},
   "source": [
    "# The basics\n",
    "This jupyter notebook will make sure that you have everything you need to run langchain and explain a little bit the basics of interacting with GPT models.\n",
    "Adapted from [LangChain's tutorial](https://python.langchain.com/v0.2/docs/tutorials/llm_chain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241e2e58-a9db-4a61-b88d-241128f95eb1",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "We will use [LangChain](https://www.langchain.com/langchain) as our SDK to interact with different LLM's. It's abstractions concerning the different models as well as easy to plug-in vector DB's and adding \"memory\" to a use-case make it one of the best tools to prototype GenAI products. For that we need to make sure we have the right packages installed.\n",
    "\n",
    "Quick note on jupyter-notebooks: \n",
    "* ctrl+enter will run the cell\n",
    "* Any cell starting with `!` will me a command that you could also run in a terminal\n",
    "* Feel free to modify the code inside them and play around with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783940cd-009c-4e3a-ae59-10d9a9030be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ericksantillan-planday/chatbot-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce99872-e948-47ae-a80a-bed07253ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r chatbot-tutorial/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b8c635-a887-4a34-8417-8d157ce32ac3",
   "metadata": {},
   "source": [
    "# Calling our GPT model\n",
    "We will use a model deployed on Azure. The way that we interact with the model is through a POST request to a specific endpoint. This is how the request looks like:\n",
    "```json\n",
    "{\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 1,\n",
    "  \"stream\": false,\n",
    "  \"stop\": null,\n",
    "  \"max_tokens\": 4096,\n",
    "  \"presence_penalty\": 0,\n",
    "  \"frequency_penalty\": 0,\n",
    "  \"logit_bias\": {},\n",
    "  \"user\": \"user-1234\",\n",
    "  \"messages\": [\n",
    "    {}\n",
    "  ],\n",
    "  \"data_sources\": [\n",
    "    {}\n",
    "  ],\n",
    "  \"n\": 1,\n",
    "  \"seed\": 1,\n",
    "  \"response_format\": {\n",
    "    \"type\": \"json_object\"\n",
    "  },\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"description\": \"string\",\n",
    "        \"name\": \"string\",\n",
    "        \"parameters\": {\n",
    "          \"additionalProp1\": {}\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"tool_choice\": \"none\",\n",
    "  \"functions\": [\n",
    "    {\n",
    "      \"name\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"parameters\": {\n",
    "        \"additionalProp1\": {}\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"function_call\": \"none\"\n",
    "}\n",
    "```\n",
    "As you can see, there are different properties and we won't go through all of them on this workshop. The most important one will be the `messages` properties which contains all the different messages from the user as well as the answers from the model.\n",
    "\n",
    "At this point, you could use whatever you want to interact with the LLM, for example, postman or curl from your terminal. This is one examle:\n",
    "\n",
    "```shell\n",
    "curl https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2024-02-01 \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"api-key: YOUR_API_KEY\" \\\n",
    "  -d '{\"messages\":[{\"role\": \"user\", \"content\": \"hello!\"}]}'\n",
    "```\n",
    "\n",
    "Please ask the moderator for the endpoint, API key and all the needed information to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfc3f2-32fd-4113-bcf0-475fbcb0cc0b",
   "metadata": {},
   "source": [
    "## Using LangChain\n",
    "The easiest way to start interacting with the API with python is to use LangChain's [AzureChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html#langchain_openai.chat_models.azure.AzureChatOpenAI). This is object inherits from `ChatModels`. They are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. This allows us also to easily change of LLM without changing the code.\n",
    "\n",
    "To just simply call the model, we can pass in a list of messages to the `.invoke()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383cd35-20f7-4dec-b826-048d8e90b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ef3a6-1207-450a-bdcb-e00b655be0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_deployment=\"\"\n",
    "api_key=\"\"\n",
    "openai_api_version=\"2024-02-01\"\n",
    "azure_endpoint=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde7262-9fad-4456-a12a-20722772a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35 = AzureChatOpenAI(\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_key=api_key,\n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4f677-cc63-469b-9948-1da00001805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efcb169-1a33-464a-b299-a150c03aed4d",
   "metadata": {},
   "source": [
    "### Messages\n",
    "We can also use messages to keep track of our inputs, and separate between `SystemMessage`, `HumanMessage` and `AIMessage`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d7407-4160-4f20-93f8-f5715b5903a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c83f1a-a343-4f24-a150-1e74e2af6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "gpt_35.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c7a21-aa08-42fb-94f2-c04ddc646f0d",
   "metadata": {},
   "source": [
    "### OutputParsers\n",
    "Notice that the response from the model is an AIMessage. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf56b70-686a-4901-9257-4415e92e739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9262f7c0-41e5-4e4d-b52c-55166e31ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "result = gpt_35.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d99dc0-2336-47c2-b6fa-ce6cd9f50c34",
   "metadata": {},
   "source": [
    "More commonly, we can \"chain\" the model with this output parser. This means this output parser will get called everytime in this chain. This chain takes on the input type of the language model (string or list of message) and returns the output type of the output parser (string).\n",
    "\n",
    "We can easily create the chain using the `|` operator. The `|` operator is used in LangChain to combine two elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e11ac-9499-4cec-ab6b-71c150be65e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  gpt_35 | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77bd195-52d7-40c6-9c06-3258ec375907",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7ed73f-9f3c-4e24-a052-d6684bb3b1c2",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\n",
    "\n",
    "PromptTemplates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n",
    "\n",
    "Let's create a PromptTemplate here. It will take in two user variables:\n",
    "\n",
    "* `language`: The language to translate text into\n",
    "* `text`: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a2b2ab-ec30-4206-b180-2c97c397e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b76985-f521-4138-9bdf-5cc0084114d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4579d-168e-4759-a744-f0d9fc04e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c48d2f-32d5-4f12-86c4-417929226a4f",
   "metadata": {},
   "source": [
    "The input to this prompt template is a dictionary (a python JSON if you want...). We can play around with this prompt template by itself to see what it does by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fb55d-dbd2-414b-90a8-c2949eceb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke({\"language\" : \"french\", \"text\": \"hello!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff184cb-d562-4555-af7e-947938f95ae4",
   "metadata": {},
   "source": [
    "We can now combine this with the model and the output parser from above. This will chain all three components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add69fcf-340c-4cff-9ab3-4f6c8d66ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | gpt_35 | parser\n",
    "\n",
    "chain.invoke({\"language\": \"french\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d73ac2-1a5a-41cf-b220-0b7281b4fd96",
   "metadata": {},
   "source": [
    "# Exercise!\n",
    "\n",
    "Make a chain that will take as an input a superhero and an animal and returns a a creative name for the new superhero-animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecd9a5b-9563-4761-8462-6280d11d8832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea4248-c402-403d-94e7-167b626f05b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "695ba09d-ecb0-4408-887d-246979d13581",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc1c765-1cc5-484a-8caa-2d1f4fb04411",
   "metadata": {},
   "source": [
    "A typical RAG application has two main components:\n",
    "* **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "* **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "In this notebook we will focus on Indexing\n",
    "\n",
    "Indexing has three main parts:\n",
    "* Load: First we need to load our data. This is done with DocumentLoaders.\n",
    "* Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "* Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5b52f-6ea1-4a64-91d5-5df47ddf0b1b",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "For this workshop we have already downloaded and preprocess some data for you. You can see it under `/docs`.\n",
    "\n",
    "## Loading data\n",
    "The data has been saved as a markdown file. We will use the DirectoryLoader. To see other type of loaders: [Document Loaders](https://python.langchain.com/v0.2/docs/how_to/#document-loaders). You may want to go ahead and try to load your own data ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9134d8-d495-44ef-bc5b-3e98b361fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4021d75f-9ce5-47a4-9022-72368bace66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"chatbot-tutorial/docs/\", loader_cls=TextLoader, glob=\"*.md\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5611132-6ed9-4621-ab85-2869bba84b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f7c7c-24ee-44e1-92f1-04d760a7bdb9",
   "metadata": {},
   "source": [
    "# Indexing: Split\n",
    "Our loaded documents are not too long, which makes this step *optional* but for larger documents it is mandatory as there can be  too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3deb2d2-c437-4e44-9352-af097b639049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f957d91-d830-42aa-97a1-faa802e509cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a786111-1955-4b77-8714-a99889627f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2e124-3619-4c2d-8f2b-d92d69631481",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab1da21-fc5a-469a-9a2e-4c6e05a39b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41791232-1ddb-41cd-9c0c-4bd4f0e9605e",
   "metadata": {},
   "source": [
    "# Indexing: Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fe0531-8610-4358-b3c8-d21d4f655e5f",
   "metadata": {},
   "source": [
    "Now we need to index our text chunks so that we can search over them at runtime. \n",
    "\n",
    "Several ways to perform this can be used, this is a simple exercise of Information Retrieval. You could use for example TF-IDF, Bag of words, etc... \n",
    "\n",
    "However, the most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "Different Embeddings can be used, we will use some local embeddings for this using a simple model called `sentence-transformers/all-mpnet-base-v2`. We will use a library called HuggingFace which has become like the \"Github\" of ML models. You can see more [here](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33af80-83da-435e-bba7-c3c252ec83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c68e0c-c8ac-46cb-8b28-8894ad011b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d3be8-be14-4d14-856d-7957560e8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2873177-d1b5-44d7-984e-fec57e82b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally for next steps\n",
    "vectorstore.save_local(\"vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b279e-8dd2-4aab-a957-8c898656a27b",
   "metadata": {},
   "source": [
    "# For the curious ones\n",
    "If you want to see how a document is stored you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb02ae42-fa26-413a-a7a3-a6e8e5c5fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca1d0a-1a05-4497-aacc-cc91ab21eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b1a24d-0f77-42c5-a0aa-5667bff53e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ba4bd-d1c1-4d76-a20a-7828f9583927",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.index.reconstruct(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b801ebf2-0db9-465f-85de-9ddf62f4c6a9",
   "metadata": {},
   "source": [
    "# RAG: putting everything together\n",
    "\n",
    "In this notebook we will put all the building blocks together to have our own RAG application\n",
    "\n",
    "## Bringing back all of our work from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c0fdde-6c0b-45ff-b105-05c2f5af1e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464120b2-25e7-4460-82f0-24f233385a6e",
   "metadata": {},
   "source": [
    "### Define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653ce3b-151c-49b2-9e37-7c9c15e8d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should allow us to change easily the LLM used\n",
    "llm=gpt_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181933b9-9f09-4ff9-a282-571bc36a1b53",
   "metadata": {},
   "source": [
    "### Load our vectorsore and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fd7d4-4367-43d8-b794-ff27ed25bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "vectorstore = FAISS.load_local(\"vector_store\", embeddings=embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5ddc8-e919-4ccc-902f-d3bd6bff0996",
   "metadata": {},
   "source": [
    "## Retrieval and Generation: Retrieve\n",
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "First we need to define our logic for searching over documents. LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.\n",
    "\n",
    "The most common type of [Retriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/) is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facilitate retrieval. Any VectorStore can easily be turned into a Retriever with `VectorStore.as_retriever()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563db51-5f3a-4bdd-9273-ff936c46e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073f8b6-76e2-40eb-807d-e8e4005358c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are revenue units?\"\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709e101-dbab-414a-aed9-5f88653a4dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f326303-f5fa-4791-a6e1-f8f17ee0280c",
   "metadata": {},
   "source": [
    "## What's happening under the hood?\n",
    "It may seem really obscur what one line of code is doing but it's really simple. It's a 4 step process:\n",
    "1. The `query` is passed through our embedding model and gets transformed into a vector, let's called it `query_vector`\n",
    "2. The `query_vector` is then compared to all the vectors in the vectorstore. Remember that those vectors in the vectorstore are just a mathematical representation of parts of the documents\n",
    "3. We then take the vectors that are the most \"similar\" to our `query_vector`\n",
    "4. We return a list with the documents that had the nearest distance to the `query`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf97e2-079c-4942-8807-0875a05f4cc6",
   "metadata": {},
   "source": [
    "# Retrieval and Generation: Generate\n",
    "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.\n",
    "\n",
    "Let's start by defining the message we will send to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593380cd-f419-4c3c-b37b-06d2678d5942",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a67c7-e16d-494e-b930-eede921c3208",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c01419-5db5-470e-b501-4aa1cdf1117a",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "We will create a chain called `rag_chain` that will have only one input: the user's `question`.\n",
    "\n",
    "The `question` be forked and passed through two different pipelines:\n",
    "1. The retrieval pipeline, where the question will be compared to the documents inside the vectorstore using the `retriever` and its output will be appended usint the `format_docs` function. The output of this chain will be a string and be passed to `prompt` on the `context` property.\n",
    "2. The `question` will be other property passed to the `prompt`.\n",
    "\n",
    "Once the prompt is filled with context and the question, we will send it to the `llm`, and we will print out the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ef7597-6495-43d2-b198-7830ae6d09c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f788bf-d5d1-487e-92b5-f22d5f617f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4d7c4-83ae-4c7e-8d8e-fe37fe83639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"what is a revenue unit?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdd0de-3a62-4677-b1c9-592d5e1ae64f",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "\n",
    "Now it's up to you, here we propose some exercises for you to play with, feel free to mess around with it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42c598-7e71-4c10-afed-9039ad9886f6",
   "metadata": {},
   "source": [
    "# Exercise 1: Validation\n",
    "Right now our agent can answer questions about Planday... but also about coding in python, or about the weather in Mexico. I think you can see how this can be abused... How can you put some guard rails to avoid it?\n",
    "\n",
    "Maybe modify the prompt... maybe separate it into two prompts... who knows\n",
    "\n",
    "The following prompt shouldn't be possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003de1d7-5fad-4ff8-8b68-8af1f254892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rag_chain.invoke(\"Write a python function that somes all fibonacci numbers between 1-18\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec062987-6a8d-462a-ba3e-ec64a8cd8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed02f4-3359-410d-886a-116dc632c244",
   "metadata": {},
   "source": [
    "# Exercise 2: Follow-up questions\n",
    "Right now, our agent can answer questions about Planday. But if you ask a follow up question, it has no idea about what you were talking about as an LLM has **no memory**. The only way to provide it with memory is by somehow adding the past requests manually to the request. How could you do it...?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d4be32-ba88-496e-a86e-982190044d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac8884-7e36-4d28-a395-ee3872c95739",
   "metadata": {},
   "source": [
    "# Exercise 3: Cite your sources!\n",
    "We know LLMs are prompt to hallucinate... how can you make it retourn the sources of where the knowledge came from?\n",
    "\n",
    "Pssst: maybe you want to look into modifyin the `format_docs` function, although there are several ways of doing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c22960-cfd5-4a06-a99b-d286d6cb24d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
