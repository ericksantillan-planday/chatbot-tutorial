{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966ab28a-8336-44de-a87b-195a13bcde61",
   "metadata": {},
   "source": [
    "# The basics\n",
    "This jupyter notebook will make sure that you have everything you need to run langchain and explain a little bit the basics of interacting with GPT models.\n",
    "Adapted from [LangChain's tutorial](https://python.langchain.com/v0.2/docs/tutorials/llm_chain/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25eedc-5b7f-4a39-8616-acefaf9f77fe",
   "metadata": {},
   "source": [
    "## Installing packages\n",
    "We will use [LangChain](https://www.langchain.com/langchain) as our SDK to interact with different LLM's. It's abstractions concerning the different models as well as easy to plug-in vector DB's and adding \"memory\" to a use-case make it one of the best tools to prototype GenAI products. For that we need to make sure we have the right packages installed.\n",
    "\n",
    "Quick note on jupyter-notebooks: \n",
    "* ctrl+enter will run the cell\n",
    "* Any cell starting with `!` will me a command that you could also run in a terminal\n",
    "* Feel free to modify the code inside them and play around with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f104744-e224-4938-9fe0-6fb940655113",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274dd01-d94f-4286-bd14-2f058342942f",
   "metadata": {},
   "source": [
    "# Calling our GPT model\n",
    "We will use a model deployed on Azure. The way that we interact with the model is through a POST request to a specific endpoint. This is how the request looks like:\n",
    "```json\n",
    "{\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 1,\n",
    "  \"stream\": false,\n",
    "  \"stop\": null,\n",
    "  \"max_tokens\": 4096,\n",
    "  \"presence_penalty\": 0,\n",
    "  \"frequency_penalty\": 0,\n",
    "  \"logit_bias\": {},\n",
    "  \"user\": \"user-1234\",\n",
    "  \"messages\": [\n",
    "    {}\n",
    "  ],\n",
    "  \"data_sources\": [\n",
    "    {}\n",
    "  ],\n",
    "  \"n\": 1,\n",
    "  \"seed\": 1,\n",
    "  \"response_format\": {\n",
    "    \"type\": \"json_object\"\n",
    "  },\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"description\": \"string\",\n",
    "        \"name\": \"string\",\n",
    "        \"parameters\": {\n",
    "          \"additionalProp1\": {}\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"tool_choice\": \"none\",\n",
    "  \"functions\": [\n",
    "    {\n",
    "      \"name\": \"string\",\n",
    "      \"description\": \"string\",\n",
    "      \"parameters\": {\n",
    "        \"additionalProp1\": {}\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"function_call\": \"none\"\n",
    "}\n",
    "```\n",
    "As you can see, there are different properties and we won't go through all of them on this workshop. The most important one will be the `messages` properties which contains all the different messages from the user as well as the answers from the model.\n",
    "\n",
    "At this point, you could use whatever you want to interact with the LLM, for example, postman or curl from your terminal. This is one examle:\n",
    "\n",
    "```shell\n",
    "curl https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/YOUR_DEPLOYMENT_NAME/chat/completions?api-version=2024-02-01 \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"api-key: YOUR_API_KEY\" \\\n",
    "  -d '{\"messages\":[{\"role\": \"user\", \"content\": \"hello!\"}]}'\n",
    "```\n",
    "\n",
    "Please ask the moderator for the endpoint, API key and all the needed information to run the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959d41f-55a1-4eab-8e12-ed33b460c331",
   "metadata": {},
   "source": [
    "## Using LangChain\n",
    "The easiest way to start interacting with the API with python is to use LangChain's [AzureChatOpenAI](https://api.python.langchain.com/en/latest/chat_models/langchain_openai.chat_models.azure.AzureChatOpenAI.html#langchain_openai.chat_models.azure.AzureChatOpenAI). This is object inherits from `ChatModels`. They are instances of LangChain \"Runnables\", which means they expose a standard interface for interacting with them. This allows us also to easily change of LLM without changing the code.\n",
    "\n",
    "To just simply call the model, we can pass in a list of messages to the `.invoke()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d4a4f-4ece-476a-b087-b885a1ed3c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118c031-cb8c-4357-acd7-566751157431",
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_deployment=\"\"\n",
    "api_key=\"\"\n",
    "openai_api_version=\"2024-02-01\"\n",
    "azure_endpoint=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bce56f-fa78-4434-aadf-9cc507ffd87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35 = AzureChatOpenAI(\n",
    "    azure_deployment=azure_deployment,\n",
    "    api_key=api_key,\n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_endpoint=azure_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41d9206-49bc-40a0-8df2-21516a38d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35.invoke(\"hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570ea85-8095-47a9-9291-dbeaada8a2da",
   "metadata": {},
   "source": [
    "### Messages\n",
    "We can also use messages to keep track of our inputs, and separate between `SystemMessage`, `HumanMessage` and `AIMessage`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1adfa-2ab1-4023-b46d-d2b115de7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04407bb-7b1d-4476-a378-adcd2413664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "gpt_35.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c64dcc-54b5-4773-8608-63caa9fb9d9a",
   "metadata": {},
   "source": [
    "### OutputParsers\n",
    "Notice that the response from the model is an AIMessage. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cc014-76d0-434c-a90d-747dfd50dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f20a8fe-74c3-4ecc-94e3-97388553f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "result = gpt_35.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c484fe-d8f3-462a-b49b-caf6cec280a9",
   "metadata": {},
   "source": [
    "More commonly, we can \"chain\" the model with this output parser. This means this output parser will get called everytime in this chain. This chain takes on the input type of the language model (string or list of message) and returns the output type of the output parser (string).\n",
    "\n",
    "We can easily create the chain using the `|` operator. The `|` operator is used in LangChain to combine two elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49196a0e-4744-47f2-9eda-da9e9930a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain =  gpt_35 | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a84cf6-1364-4f97-8d89-be14797360c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3df25-c986-4c1a-bff4-46df59ffeea2",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\n",
    "\n",
    "PromptTemplates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model.\n",
    "\n",
    "Let's create a PromptTemplate here. It will take in two user variables:\n",
    "\n",
    "* `language`: The language to translate text into\n",
    "* `text`: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f714be-07ea-4efa-912c-d56e0e07d063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538bf0f-b735-4208-920d-30635aa58d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e1a3e1-5d2b-4111-b82e-9dd78d1cf4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e917551-e381-469b-bb2b-1e14c5f55936",
   "metadata": {},
   "source": [
    "The input to this prompt template is a dictionary (a python JSON if you want...). We can play around with this prompt template by itself to see what it does by itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6355e26d-ea89-46c2-94c1-c88012f64a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template.invoke({\"language\" : \"french\", \"text\": \"hello!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54457bf4-0d82-406f-97d8-189a503b81d7",
   "metadata": {},
   "source": [
    "We can now combine this with the model and the output parser from above. This will chain all three components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ca67f-6a32-4dd0-9a6d-bdcd8a779a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | gpt_35 | parser\n",
    "\n",
    "chain.invoke({\"language\": \"french\", \"text\": \"hi\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1711e-b47d-4855-b62b-944ad0491158",
   "metadata": {},
   "source": [
    "# Exercise!\n",
    "\n",
    "Make a chain that will take as an input a superhero and an animal and returns a a creative name for the new superhero-animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58074403-b38f-471c-8651-a1b5a3d5e498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
