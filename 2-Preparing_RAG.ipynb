{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a42ab3-5c74-446c-a4ab-8a5f7627fc14",
   "metadata": {},
   "source": [
    "# Preparing for RAG: data and vector embeddings\n",
    "\n",
    "Adapted from [LangChain's RAG Tutorial](https://python.langchain.com/v0.2/docs/tutorials/rag/)\n",
    "\n",
    "One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.\n",
    "\n",
    "This tutorial will show how to build a simple Q&A application over a text data source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9717217-624e-4598-842c-b5401183ae3e",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "RAG is a technique for augmenting LLM knowledge with additional data.\n",
    "\n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).\n",
    "\n",
    "LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bf4616-28ef-42fd-97a8-9404bbd3b78d",
   "metadata": {},
   "source": [
    "A typical RAG application has two main components:\n",
    "* **Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.\n",
    "\n",
    "* **Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "In this notebook we will focus on Indexing\n",
    "\n",
    "Indexing has three main parts:\n",
    "* Load: First we need to load our data. This is done with DocumentLoaders.\n",
    "* Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "* Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0629c1b-3ee6-4361-b40e-c9f3352dea02",
   "metadata": {},
   "source": [
    "## Getting data\n",
    "For this workshop we have already downloaded and preprocess some data for you. You can see it under `/docs`.\n",
    "\n",
    "## Loading data\n",
    "The data has been saved as a markdown file. We will use the DirectoryLoader. To see other type of loaders: [Document Loaders](https://python.langchain.com/v0.2/docs/how_to/#document-loaders). You may want to go ahead and try to load your own data ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769e009-db62-4440-8cfd-95a7dc0ab7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6214aa-e80b-4a66-9519-3e83895796e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\"docs/\", loader_cls=TextLoader, glob=\"*.md\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04687077-b08a-4041-bec5-1846be2993a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8acc64-4e89-492d-a41f-d3b5196ffb00",
   "metadata": {},
   "source": [
    "# Indexing: Split\n",
    "Our loaded documents are not too long, which makes this step *optional* but for larger documents it is mandatory as there can be  too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f211eedf-3b0e-4f7b-a53f-41a52b819552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9abc97-a686-4a35-8544-b32eeb46fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e81fa-d1c0-4e03-ab82-cec6981b1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43295b2-b2c4-4501-a903-b6330304dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2899cf-e1dc-468d-b67f-391835c4b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5008c3-a8be-4342-9b55-0648278b4c6e",
   "metadata": {},
   "source": [
    "# Indexing: Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee12c76-818e-42ba-ac17-fd09e3ba58aa",
   "metadata": {},
   "source": [
    "Now we need to index our text chunks so that we can search over them at runtime. \n",
    "\n",
    "Several ways to perform this can be used, this is a simple exercise of Information Retrieval. You could use for example TF-IDF, Bag of words, etc... \n",
    "\n",
    "However, the most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "Different Embeddings can be used, we will use some local embeddings for this using a simple model called `sentence-transformers/all-mpnet-base-v2`. We will use a library called HuggingFace which has become like the \"Github\" of ML models. You can see more [here](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5542f18-a5d1-43bf-a1e0-fcea5d03dc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb8665-72bd-4723-8454-7cd53143b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0301b5-fa87-42df-8ee6-4b05cf457e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(documents=all_splits, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e6fe1-7b0b-4f8d-9c32-855ea5897813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save locally for next steps\n",
    "vectorstore.save_local(\"vector_store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d944a9c-2853-4485-8b78-0f0c73d46f60",
   "metadata": {},
   "source": [
    "# For the curious ones\n",
    "If you want to see how a document is stored you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83349d8a-13d4-4ba1-99c1-69a4a13cd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_splits[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d4cef0-489d-48c4-8613-ad6f37a6963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b275ab7-06c7-4c31-b0bf-7e177ad880e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02943b2-feda-41c5-9acd-c38bfc214535",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.index.reconstruct(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notebooks",
   "language": "python",
   "name": "notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
